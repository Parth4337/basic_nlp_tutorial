{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sequence to Sequence Models (COSC 6336)\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right; font-size: large;\"><i>authored by Gustavo Aguilar</i></div>\n",
    "<div style=\"text-align: right; font-size: small;\"><i>March 19, 2020</i></div>\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we show a minimalistic implementation of sequence to sequence (seq2seq) models. We use LSTM-based encoder and decoder, and we optimize the model on a toy dataset from automatically-generated data. The high-level view of the model is shown in the image below:\n",
    "\n",
    "<img src='images/encoder-decoder.png' width='70%'/>\n",
    "\n",
    "Then, we improve our model using an attention-based decoder, as described in the [slides of the lecture](https://docs.google.com/presentation/d/1nmTSrwa-8Pi456rBYfTyIpx9Gz_Kc1WBpBOf0SC4vCI/edit?usp=sharing). \n",
    "\n",
    "Here's what we cover in this notebook:\n",
    "1. Define a simple (auto-generated) dataset\n",
    "2. Define the encoder \n",
    "3. Define the decoder \n",
    "4. Train the model\n",
    "5. Evaluate and inspect the predictions\n",
    "6. Compare with seq2seq with attention\n",
    "\n",
    "\n",
    "Lastly, you will find the **assignment** at the end of the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define a simple (auto-generated) dataset\n",
    "\n",
    "Our automatically-generated data will contain sequences with repeated and unordered letters. Such sequences must be mapped to alphabetically-sorted sequences of unique letters. Here are a few examples:\n",
    "```\n",
    "ccccaaabb        ->   abc\n",
    "vvvrxduuu        ->   druvx\n",
    "sddvvvzzuuuxxx   ->   dsuvxz\n",
    "```\n",
    "\n",
    "Note that this dataset will pose the problems mentioned during the lecture to a vanilla seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have two splits in our data, one for **training**, and another for validation. The training set will be **20,000** samples and we will use this set to update the parameters of the model. The **validation** set will be **5,000** samples, which we use to select the best model.\n",
    "\n",
    "_**NOTE**: The validation set is never used to update the parameters of the model. Instead, we use it to make sure that the model is generalizing well, and not doing overfitting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "    \n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(20_000)\n",
    "valid_inp, valid_out = sorting_letters_dataset(5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to map the text data into numeric values. These numeric values are indexes that correspond to the entries in the embedding lookup table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the encoder\n",
    "\n",
    "Our encoder will be a simple LSTM model with one layer and one direction. \n",
    "\n",
    "<img src='images/encoder.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (emb): Embedding(27, 64)\n",
       "  (lstm): LSTM(64, 128, batch_first=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        seq = torch.tensor([inputs]).to(device) # (1, seqlen)\n",
    "        emb = self.emb(seq) # (1, seqlen, emb_dim)\n",
    "        emb = self.drop(emb) \n",
    "        \n",
    "        outs, (h_n, c_n) = self.lstm(emb)\n",
    "        \n",
    "        if self.z_index == 1:\n",
    "            return h_n[0], c_n[0] # (seqlen, lstm_dim)\n",
    "        else:\n",
    "            return outs # (1, seqlen, lstm_dim)\n",
    "\n",
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=1)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define de decoder\n",
    "\n",
    "Similar to the encoder, the decoder will be a LSTM cell with one layer and one direction. \n",
    "\n",
    "<img src='images/decoder.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (emb): Embedding(29, 64)\n",
       "  (lstm): LSTMCell(64, 128)\n",
       "  (clf): Linear(in_features=128, out_features=29, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (objective): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.clf = nn.Linear(lstm_size, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, state, targets, curr_token, last_token):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        loss = 0\n",
    "        shifted = targets + [last_token]\n",
    "        for i in range(len(shifted)):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            \n",
    "            emb = self.emb(inp)\n",
    "            emb = self.drop(emb)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i)\n",
    "\n",
    "            scores = self.clf(q_i)\n",
    "            target = torch.tensor([shifted[i]]).to(device)\n",
    "            loss += self.objective(scores, target)\n",
    "            \n",
    "            curr_token = shifted[i]\n",
    "            \n",
    "        return loss / len(shifted)\n",
    "\n",
    "    def predict(self, state, curr_token, last_token, maxlen):\n",
    "        device = next(self.parameters()).device\n",
    "        preds = []\n",
    "        for i in range(maxlen):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            emb = self.emb(inp)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            h_i, _ = state\n",
    "            \n",
    "            scores = self.clf(h_i)\n",
    "            pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "            curr_token = pred\n",
    "            \n",
    "            if last_token == pred:\n",
    "                break\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also consider a decoder LSTM with attention\n",
    "\n",
    "The attention version that we use for this implementation is Luong's attention, also known as multiplicative attention. \n",
    "\n",
    "Consider the encoder outputs $h = [h_1, h_2, \\dots, h_n]$, and the query vector $q_j$ of the decoding time step $j$ as the hidden vector of the decoder LSTM, then we define multiplicative attention as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_i &= v^\\intercal tanh(W [h_i + q_j]) \\\\\n",
    "\\alpha_i &= \\frac{exp(u_i)}{\\sum^N_k exp(u_k)} \\\\\n",
    "c &= \\sum^N_i \\alpha_i h_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attn_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, attn_dim)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs):\n",
    "        # enc_outs -> (batch, seqlen, hidden)\n",
    "        # dec_hidden -> (batch, hidden)\n",
    "        \n",
    "        seqlen = enc_outs.size(1)\n",
    "        \n",
    "        repeat_h = dec_hidden.unsqueeze(1)  # make room to repeat on seqlen dim\n",
    "        repeat_h = repeat_h.repeat(1, seqlen, 1)  # (1, seqlen, hidden)\n",
    "\n",
    "        concat_h = torch.cat((enc_outs, repeat_h), dim=2) # (1, seqlen, hidden*2)\n",
    "        \n",
    "        scores = self.v(torch.tanh(self.W(concat_h))) # (1, seqlen, 1)\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        weighted = enc_outs * probs # (1, seqlen, hidden)\n",
    "        \n",
    "        context = torch.sum(weighted, dim=1, keepdim=False) # (1, hidden)\n",
    "        combined = torch.cat((dec_hidden, context), dim=1)  # (1, hidden*2)\n",
    "        \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want the decoder to focus on the right hidden outputs of the encoder, we need the to modify the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionDecoder(\n",
       "  (emb): Embedding(29, 64)\n",
       "  (lstm): LSTMCell(64, 128)\n",
       "  (attn): Attention(\n",
       "    (W): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
       "  )\n",
       "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (objective): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, attn_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.attn = Attention(lstm_size * 2, attn_size)\n",
    "        self.clf = nn.Linear(lstm_size * 2, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \n",
    "    def init_state(self, device):\n",
    "        h_0 = torch.zeros(1, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        c_0 = torch.zeros(1, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        return h_0, c_0\n",
    "        \n",
    "    def forward(self, enc_outs, targets, curr_token, last_token):\n",
    "        loss = 0\n",
    "        device = enc_outs.device\n",
    "        state = self.init_state(device)\n",
    "        \n",
    "        shifted = targets + [last_token]\n",
    "        for i in range(len(shifted)):\n",
    "            inp = torch.tensor([curr_token]).to(device) # (1,)\n",
    "            \n",
    "            emb = self.emb(inp) # (1, emb_dim)\n",
    "            emb = self.drop(emb)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i) # (1, emb_dim)\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs)\n",
    "            \n",
    "            scores = self.clf(combined)\n",
    "            target = torch.tensor([shifted[i]]).to(device)\n",
    "            loss += self.objective(scores, target)\n",
    "            \n",
    "            curr_token = shifted[i]\n",
    "            \n",
    "        return loss / len(shifted)\n",
    "\n",
    "    def predict(self, enc_outs, curr_token, last_token, maxlen):\n",
    "        preds = []\n",
    "        device = enc_outs.device\n",
    "        state = self.init_state(device)\n",
    "        \n",
    "        for i in range(maxlen):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            emb = self.emb(inp)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs)\n",
    "            \n",
    "            scores = self.clf(combined)\n",
    "            pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "            curr_token = pred\n",
    "            \n",
    "            if last_token == pred:\n",
    "                break\n",
    "                \n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128, attn_size=100)\n",
    "decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the encoder-decoder model\n",
    "\n",
    "During training, we pass the targets to the decoder so that it can be used as the ideal input at time step $i$, instead of using the decoder predictions of the previous time step $i-1$.\n",
    "\n",
    "<img src='images/encoder-decoder-lstm.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']\n",
    "\n",
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "\n",
    "def train(encoder, decoder, train_x, train_y, batch_size=50, epochs=10, print_every=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.99)\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.zero_grad(); enc_optim.zero_grad()\n",
    "        decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "        train_x, train_y = shuffle(train_x, train_y)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        batch_loss = 0    \n",
    "        \n",
    "        for i in range(len(train_x)):\n",
    "            x = train_x[i]\n",
    "            y = train_y[i]\n",
    "            \n",
    "            batch_loss += decoder(encoder(x), y, START_IX, STOP_IX)\n",
    "\n",
    "            if (i+1) % batch_size == 0:\n",
    "                batch_loss.backward()\n",
    "                enc_optim.step()\n",
    "                dec_optim.step()\n",
    "\n",
    "                encoder.zero_grad(); enc_optim.zero_grad()\n",
    "                decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "                epoch_loss += batch_loss.item()\n",
    "                batch_loss = 0\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"**** Epoch {epoch} - Loss: {epoch_loss / len(train_x):.6f} ****\")\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the vanilla seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Decoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (clf): Linear(in_features=128, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n",
      "**** Epoch 1 - Loss: 1.991759 ****\n",
      "**** Epoch 2 - Loss: 0.853610 ****\n",
      "**** Epoch 3 - Loss: 0.456747 ****\n",
      "**** Epoch 4 - Loss: 0.270997 ****\n",
      "**** Epoch 5 - Loss: 0.174615 ****\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=1)\n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "encoder, decoder = train(encoder, decoder, train_x, train_y, batch_size=50, epochs=5, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict()}, 'seq2seq.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate and inspect the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, samples, index_to_elem):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    preds = []\n",
    "    for i in range(len(samples)):\n",
    "        pred = decoder.predict(encoder(samples[i]), START_IX, STOP_IX, maxlen=10)\n",
    "        pred = [index_to_elem[ix] for ix in pred]\n",
    "        preds.append(''.join(pred))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.978"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(encoder, decoder, valid_x, tgt_vocab.itos)\n",
    "groundtruth = [''.join(t) for t in valid_out]\n",
    "\n",
    "accuracy_score(groundtruth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjuvvmmmauuu --> ajmuv --> ajmuv\n",
      "ceeeppnnn --> cenp --> cenp\n",
      "iiikkxxaaijjjnnaaeee --> aeijknx --> aeijknx\n",
      "hqqtttnnnooawwc --> achnoqtw --> achnoqtw\n",
      "zzooeeeooonnzkhjhhh --> ehjknoz --> ehjknoz\n",
      "aaaepppiitmbbb --> abeimpt --> abeimpt\n",
      "wssrriidyyyf --> dfirswy --> dfirswy\n",
      "kkuhhcrrzzmmmwww --> chkmruwz --> chkmruwz\n",
      "cssxx --> csx --> csx\n",
      "ddooouulllelllc --> cdelou --> cdelou\n",
      "lllkkkaaaggfihrrr --> afghiklr --> afghklr\n",
      "ttgbbiillzocccuuu --> bcgilotuz --> bcgilotuz\n",
      "lliiieeexiivvvtttrrrfffxx --> efilrtvx --> efilrtvx\n",
      "parhhhylll --> ahlpry --> ahlpry\n",
      "qqqccciialrrrvvvlzzz --> acilqrvz --> acilqrvz\n",
      "cczppkq --> ckpqz --> ckpqz\n",
      "ffffffsaa --> afs --> afs\n",
      "kooopppwooeet --> ekoptw --> ekoptw\n",
      "oyyyiiio --> ioy --> ioy\n",
      "nnnyyxxuuufff --> fnuxy --> fnuxy\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(valid_inp[:20])):\n",
    "    x = ''.join(valid_inp[i])\n",
    "    y = ''.join(valid_out[i])\n",
    "    \n",
    "    print(f\"{x} --> {y} --> {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with seq2seq with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "AttentionDecoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (attn): Attention(\n",
      "    (W): Linear(in_features=256, out_features=100, bias=True)\n",
      "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
      "  )\n",
      "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n",
      "**** Epoch 1 - Loss: 1.433182 ****\n",
      "**** Epoch 2 - Loss: 0.375678 ****\n",
      "**** Epoch 3 - Loss: 0.209631 ****\n",
      "**** Epoch 4 - Loss: 0.183923 ****\n",
      "**** Epoch 5 - Loss: 0.147630 ****\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=0)\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128, attn_size=100)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "encoder, decoder = train(encoder, decoder, train_x, train_y, batch_size=50, epochs=5, print_every=1)\n",
    "\n",
    "torch.save({'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict()}, 'seq2seq+attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9816"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(encoder, decoder, valid_x, tgt_vocab.itos)\n",
    "groundtruth = [''.join(t) for t in valid_out]\n",
    "\n",
    "accuracy_score(groundtruth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjuvvmmmauuu --> ajmuv --> ajmuv\n",
      "ceeeppnnn --> cenp --> cenp\n",
      "iiikkxxaaijjjnnaaeee --> aeijknx --> aeijknx\n",
      "hqqtttnnnooawwc --> achnoqtw --> achnoqtw\n",
      "zzooeeeooonnzkhjhhh --> ehjknoz --> ehjknoz\n",
      "aaaepppiitmbbb --> abeimpt --> abeimpt\n",
      "wssrriidyyyf --> dfirswy --> dfirswy\n",
      "kkuhhcrrzzmmmwww --> chkmruwz --> chkmruwz\n",
      "cssxx --> csx --> csx\n",
      "ddooouulllelllc --> cdelou --> cdelou\n",
      "lllkkkaaaggfihrrr --> afghiklr --> afghiklr\n",
      "ttgbbiillzocccuuu --> bcgilotuz --> bcgilotuz\n",
      "lliiieeexiivvvtttrrrfffxx --> efilrtvx --> efilrtvx\n",
      "parhhhylll --> ahlpry --> ahlpry\n",
      "qqqccciialrrrvvvlzzz --> acilqrvz --> acilqrvz\n",
      "cczppkq --> ckpqz --> ckpqz\n",
      "ffffffsaa --> afs --> afs\n",
      "kooopppwooeet --> ekoptw --> ekoptw\n",
      "oyyyiiio --> ioy --> ioy\n",
      "nnnyyxxuuufff --> fnuxy --> fnuxy\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(valid_inp[:20])):\n",
    "    x = ''.join(valid_inp[i])\n",
    "    y = ''.join(valid_out[i])\n",
    "    \n",
    "    print(f\"{x} --> {y} --> {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "As you can see, the models are quite good at the task at hand. However, they are not perfect and you will see that the performance on the test data drops. One of the reasons is that the models were trained for a few epochs. Even though training for longer could improve the models, the code is not efficient because it does not use batches. Instead, we are iterating sample by sample without taking advantage of parallelization. Once you optimize this code, you will see that both models are good, and the attention brings a substantial boost in performance.\n",
    "\n",
    "### Your task\n",
    "\n",
    "Your task is to make this model efficient using batches. You can keep the same overall architecture, which is a single LSTM layer with one direction on both the encoder and decoder while also using attention. **As requirement**, you must use RNN-based models with attention, but the number of layers, directions, and hidden units of the RNN are up to you. Similarly, the attention method (either any version of Luong's attention or Bahdanau's attention) can be defined by you. You can also use self-attention if you prefer, but that is not required nor needed to achieve good results.\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "We will measure the performance of your model using accuracy for the exact matches. Since this is a toy dataset, it's reasonable to measure the performance using this metric (it is worth mentioning that for tasks such as machine trasnlation, the official metric is BLEU). We will provide a test dataset (although auto-generated too) that is fixed for everyone to make all the systems comparable.\n",
    "\n",
    "### Delivery\n",
    "\n",
    "* The encoder and decoder models (a single binary file named `model.pt`)\n",
    "* The solution notebook as HTML or PDF named `notebook.html` or `notebook.pdf`\n",
    "    * displaying the epoch losses during training\n",
    "    * showing the last cell in the notebook with the resulting accuracy on the test set. You should make this cell self-contained, meaning that it loads the test set, generates the predictions, and computes the accuracy.\n",
    "\n",
    "\n",
    "### A few words on the solution\n",
    "\n",
    "A possible solution of the assignment uses the exact same architectures (same LSTM and attention mechanisms and parameters), but it was trained on batches. An iteration over the entire training set (a.k.a. epoch) took around 12 seconds with 50 samples per batch, which is 400 backpropagation steps from the 20k training samples. We trained the model for 100 epochs (about 20 minutes on a GPU) and reached over 90% of accuracy on the test set (90.06% for seq2seq and 97.04% for seq2seq+attention). \n",
    "\n",
    "### The test data\n",
    "\n",
    "The test data is generated in the same manner as the training/validation data, but with intentionally longer input sentences. Here's how the current models perform on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wwwttttjjjhhddmmmmqqffmmmdddcffu', 'cdfhjmqtuw')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv('test.txt', delimiter='\\t', header=None, usecols=[0,1])\n",
    "test_inp, test_out = test_data[0], test_data[1]\n",
    "\n",
    "test_inp[0], test_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = map_many_elems(test_inp, src_vocab.stoi)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8784"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=1)\n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128)\n",
    "\n",
    "state_dict = torch.load('seq2seq.pt')\n",
    "encoder.load_state_dict(state_dict['encoder'])\n",
    "decoder.load_state_dict(state_dict['decoder'])\n",
    "\n",
    "predictions = predict(encoder, decoder, test_x, tgt_vocab.itos)\n",
    "groundtruth = [''.join(t) for t in test_out]\n",
    "\n",
    "accuracy_score(groundtruth, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq + Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=0)\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128, attn_size=100)\n",
    "\n",
    "state_dict = torch.load('seq2seq+attn.pt')\n",
    "encoder.load_state_dict(state_dict['encoder'])\n",
    "decoder.load_state_dict(state_dict['decoder'])\n",
    "\n",
    "predictions = predict(encoder, decoder, test_x, tgt_vocab.itos)\n",
    "groundtruth = [''.join(t) for t in test_out]\n",
    "\n",
    "accuracy_score(groundtruth, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
