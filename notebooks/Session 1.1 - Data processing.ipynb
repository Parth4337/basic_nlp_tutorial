{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PROJ_DIR = os.path.join(os.environ['WORKSPACE'], 'tutorial/')\n",
    "\n",
    "if PROJ_DIR not in sys.path:\n",
    "    sys.path.append(PROJ_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (most likely) already downloaded\r\n"
     ]
    }
   ],
   "source": [
    "!cd .. \\\n",
    "&& [ ! -f aclImdb_v1.tar.gz ] \\\n",
    "&& wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \\\n",
    "&& tar -xzf  aclImdb_v1.tar.gz || echo \"Data (most likely) already downloaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the files and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "random.seed(2)\n",
    "\n",
    "def read_files(datadir, sentiment, maxlen):\n",
    "    sent_dir = os.path.join(datadir, sentiment)\n",
    "    \n",
    "    tokens = [word_tokenize(open(os.path.join(sent_dir, sent_file)).read())[:maxlen]\n",
    "              for sent_file in os.listdir(sent_dir)\n",
    "              if sent_file.endswith('.txt')]\n",
    "    labels = [sentiment] * len(tokens)\n",
    "    \n",
    "    return tokens, labels\n",
    "    \n",
    "def shuffle(tokens, labels):\n",
    "    z = list(zip(tokens, labels))\n",
    "    random.shuffle(z)\n",
    "    return zip(*z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IMDBDatset(Dataset):\n",
    "    def __init__(self, datadir, maxlen=64):\n",
    "        assert os.path.exists(datadir), datadir\n",
    "        \n",
    "        self.tokens = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.label_to_index = {'pos': 1, 'neg': 0}\n",
    "        \n",
    "        pos_tokens, pos_labels = read_files(datadir, 'pos', maxlen)\n",
    "        neg_tokens, neg_labels = read_files(datadir, 'neg', maxlen)\n",
    "        \n",
    "        self.tokens.extend(pos_tokens + neg_tokens)\n",
    "        self.labels.extend(pos_labels + neg_labels)\n",
    "        \n",
    "        self.tokens, self.labels = shuffle(self.tokens, self.labels)\n",
    "        self.labels = [self.label_to_index[label] for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.tokens[item], self.labels[item]\n",
    "    \n",
    "    def split_data(self, size):\n",
    "        dataset = copy.deepcopy(self)\n",
    "        dataset.tokens = dataset.tokens[-size:]\n",
    "        dataset.labels = dataset.labels[-size:]\n",
    "\n",
    "        self.tokens = self.tokens[:-size]\n",
    "        self.labels = self.labels[:-size]\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = IMDBDatset(os.path.join(PROJ_DIR, 'aclImdb/train'))\n",
    "test = IMDBDatset(os.path.join(PROJ_DIR, 'aclImdb/test'))\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['This', 'slick', 'and', 'gritty', 'film', 'consistently', 'delivers', '.', 'It', \"'s\", 'one', 'of', 'Frankenheimer', \"'s\", 'best', 'and', 'most', 'underrated', 'films', 'and', 'it', \"'s\", 'easily', 'the', 'best', 'Elmore', 'Leonard', 'adaptation', 'to', 'date', '(', 'and', 'if', 'you', 'are', 'scratching', 'your', 'head', 'thinking', '``', 'but', 'I', 'loved', 'GET', 'SHORTY', \"''\", 'you', 'need', 'to', 'be', 'punched', 'in', 'the', 'face', ')', '.', 'In', 'my', 'opinion', ',', 'no', 'one', 'captures', 'the'], 1)\n"
     ]
    }
   ],
   "source": [
    "print(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = train.split_data(10000) # Drop some data \n",
    "_ = test.split_data(15000) # Drop some data \n",
    "\n",
    "dev = train.split_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(dev), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1718 0\n",
      "['Ming', 'The', 'Merciless', 'does', 'a', 'little', 'Bardwork', 'and', 'a', 'movie', 'most', 'foul', '!']\n",
      "\n",
      "4895 1\n",
      "['Adrian', 'Pasdar', 'is', 'excellent', 'is', 'this', 'film', '.', 'He', 'makes', 'a', 'fascinating', 'woman', '.']\n",
      "\n",
      "5336 0\n",
      "['This', 'movie', 'is', 'terrible', 'but', 'it', 'has', 'some', 'good', 'effects', '.']\n",
      "\n",
      "8134 1\n",
      "['This', 'is', 'the', 'definitive', 'movie', 'version', 'of', 'Hamlet', '.', 'Branagh', 'cuts', 'nothing', ',', 'but', 'there', 'are', 'no', 'wasted', 'moments', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train)):\n",
    "    tokens, label = train[i]\n",
    "    if len(tokens) <= 20:\n",
    "        print(i, label)\n",
    "        print(tokens)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump it to avoid processing again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "corpus = {\n",
    "    'train': train,\n",
    "    'dev': dev,\n",
    "    'test': test\n",
    "}\n",
    "\n",
    "with open('data.pickle', 'wb') as fp:\n",
    "    pickle.dump(corpus, fp)\n",
    "\n",
    "with open('data.pickle', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5000, 10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus['train']), len(corpus['dev']), len(corpus['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
