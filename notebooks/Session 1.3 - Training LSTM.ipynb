{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PROJ_DIR = os.path.join(os.environ['WORKSPACE'], 'tutorial/')\n",
    "\n",
    "if PROJ_DIR not in sys.path:\n",
    "    sys.path.append(PROJ_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.dataset import IMDBDatset\n",
    "from src.utilities import flatten\n",
    "\n",
    "with open('data.pickle', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "def collate(batch):\n",
    "    tokens, labels = zip(*batch)\n",
    "    targets = torch.tensor(labels, dtype=torch.long)\n",
    "    return tokens, targets\n",
    "\n",
    "def get_dataloader(dataset, batch_size, shuffle=False):\n",
    "    sampler = RandomSampler(dataset) if shuffle else SequentialSampler(dataset)\n",
    "    dloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size, collate_fn=collate)\n",
    "    return dloader\n",
    "\n",
    "dataloaders = {\n",
    "    'train': get_dataloader(corpus['train'], batch_size=32, shuffle=True),\n",
    "    'dev':   get_dataloader(corpus['dev'],   batch_size=128, shuffle=False),\n",
    "    'test':  get_dataloader(corpus['test'],  batch_size=128, shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset will vary every time we iterate it because it uses `RandomSampler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders['train']:\n",
    "    print(\"Batch size:\", len(batch[0]))\n",
    "    print(batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the dev and test datasets will keep in the same order because of the `SequentialSampler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders['dev']:\n",
    "    print(\"Batch size:\", len(batch[0]))\n",
    "    print(batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(21695, 100)\n",
       "  )\n",
       "  (extractor): LSTMLayer(\n",
       "    (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (xentropy): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.nets.embedder import WordEmbedder\n",
    "from src.nets.lstm import LSTMLayer\n",
    "from src.nets.classifier import LSTMClassifier\n",
    "\n",
    "vocab = set(flatten(corpus['train'].tokens + corpus['dev'].tokens))\n",
    "\n",
    "def create_lstm_classifier():\n",
    "    embedder = WordEmbedder(vocab, os.path.join(PROJ_DIR, 'glove.6B/glove.6B.100d.txt'))\n",
    "    lstm_layer = LSTMLayer(embedder.emb_dim, hidden_dim=64, bidirectional=True, num_layers=2)\n",
    "    lstm_model = LSTMClassifier(embedder, lstm_layer)\n",
    "    return lstm_model\n",
    "\n",
    "model = create_lstm_classifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from src.utilities import process_logits\n",
    "\n",
    "def track_best_model(model_path, model, epoch, best_acc, dev_acc, dev_loss):\n",
    "    if best_acc > dev_acc:\n",
    "        return best_acc, ''\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'acc': dev_acc,\n",
    "        'loss': dev_loss,\n",
    "        'model': model.state_dict()\n",
    "    }\n",
    "    torch.save(state, model_path)\n",
    "    return dev_acc, ' * '\n",
    "\n",
    "\n",
    "def train(model, dataloaders, optimizer, config):\n",
    "    best_acc = 0\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        epoch_msg = f'E{epoch:03d}'\n",
    "        epoch_track = ''\n",
    "        \n",
    "        for dataset in dataloaders:\n",
    "            if dataset == 'train':\n",
    "                model.train()\n",
    "                model.zero_grad()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0\n",
    "            preds, truth = [], []\n",
    "\n",
    "            # ========================================================================\n",
    "            for batch_i, (tokens, targets) in enumerate(dataloaders[dataset]):\n",
    "                result = model(tokens, targets)\n",
    "                loss = result['loss']\n",
    "                \n",
    "                if dataset == 'train':\n",
    "                    loss.backward() \n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    model.zero_grad()\n",
    "\n",
    "                epoch_loss += loss.item() * len(targets)\n",
    "                batch_preds, _ = process_logits(result['output'])\n",
    "\n",
    "                preds += batch_preds\n",
    "                truth += targets.data.cpu().tolist()\n",
    "            # ========================================================================\n",
    "            \n",
    "            epoch_acc = accuracy_score(truth, preds)\n",
    "            epoch_loss /= len(dataloaders[dataset].dataset)\n",
    "            epoch_msg += ' [{}] Loss: {:.4f}, Acc: {:.4f}'.format(dataset.upper(), epoch_loss, epoch_acc)\n",
    "            \n",
    "            if dataset == 'dev':\n",
    "                best_acc, epoch_track = track_best_model(config['checkpoint'], model, epoch, best_acc, epoch_acc, epoch_loss)\n",
    "\n",
    "        print(epoch_msg + epoch_track)\n",
    "    print(\"Done training!\")\n",
    "    \n",
    "    state = torch.load(config['checkpoint'])\n",
    "    model.load_state_dict(state['model'])\n",
    "    \n",
    "    print('Returning best model from epoch {} with loss {:.5f} and accuracy {:.5f}'.format(\n",
    "        state['epoch'], state['loss'], state['acc']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E001 [TRAIN] Loss: 0.6943, Acc: 0.5145 [DEV] Loss: 0.6926, Acc: 0.4942 [TEST] Loss: 0.6922, Acc: 0.5021 * \n",
      "E002 [TRAIN] Loss: 0.6803, Acc: 0.5567 [DEV] Loss: 0.6888, Acc: 0.5184 [TEST] Loss: 0.6957, Acc: 0.5081 * \n",
      "E003 [TRAIN] Loss: 0.6465, Acc: 0.6233 [DEV] Loss: 0.6905, Acc: 0.5972 [TEST] Loss: 0.6745, Acc: 0.5980 * \n",
      "E004 [TRAIN] Loss: 0.5818, Acc: 0.6942 [DEV] Loss: 0.5376, Acc: 0.7318 [TEST] Loss: 0.5385, Acc: 0.7234 * \n",
      "E005 [TRAIN] Loss: 0.5194, Acc: 0.7412 [DEV] Loss: 0.5141, Acc: 0.7478 [TEST] Loss: 0.5202, Acc: 0.7392 * \n",
      "E006 [TRAIN] Loss: 0.4884, Acc: 0.7595 [DEV] Loss: 0.5008, Acc: 0.7522 [TEST] Loss: 0.5019, Acc: 0.7439 * \n",
      "E007 [TRAIN] Loss: 0.4719, Acc: 0.7755 [DEV] Loss: 0.5117, Acc: 0.7490 [TEST] Loss: 0.5136, Acc: 0.7413\n",
      "E008 [TRAIN] Loss: 0.4459, Acc: 0.7964 [DEV] Loss: 0.5198, Acc: 0.7468 [TEST] Loss: 0.5195, Acc: 0.7428\n",
      "E009 [TRAIN] Loss: 0.4178, Acc: 0.8115 [DEV] Loss: 0.5471, Acc: 0.7456 [TEST] Loss: 0.5445, Acc: 0.7452\n",
      "E010 [TRAIN] Loss: 0.3823, Acc: 0.8357 [DEV] Loss: 0.5542, Acc: 0.7578 [TEST] Loss: 0.5650, Acc: 0.7430 * \n",
      "Done training!\n",
      "Returning best model from epoch 10 with loss 0.55416 and accuracy 0.75780\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "config = {\n",
    "    'lr': 1e-2,\n",
    "    'momentum': 0.99,\n",
    "    'epochs': 10,\n",
    "    'checkpoint': 'lstm_model.pt'\n",
    "}\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'])\n",
    "model = train(model, dataloaders, optimizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
