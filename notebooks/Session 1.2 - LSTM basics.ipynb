{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PROJ_DIR = os.path.join(os.environ['WORKSPACE'], 'tutorial/')\n",
    "\n",
    "if PROJ_DIR not in sys.path:\n",
    "    sys.path.append(PROJ_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data to get the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 45572\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from src.dataset import IMDBDatset\n",
    "from src.utilities import flatten\n",
    "\n",
    "with open('data.pickle', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)\n",
    "    \n",
    "vocab = set(flatten(corpus['train'].tokens + corpus['dev'].tokens))\n",
    "print(\"Vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe vectors (most likely) already downloaded\r\n"
     ]
    }
   ],
   "source": [
    "!cd .. && [ ! -f glove.6B.zip ] \\\n",
    "&& wget http://nlp.stanford.edu/data/glove.6B.zip \\\n",
    "&& mkdir glove.6B \\\n",
    "&& tar -xzf glove.6B.zip -C glove.6B || echo \"GloVe vectors (most likely) already downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab, glove_file):\n",
    "        super(WordEmbedder, self).__init__()\n",
    "        assert os.path.exists(glove_file) and glove_file.endswith('.txt'), glove_file\n",
    "        \n",
    "        self.emb_dim = None\n",
    "        \n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        \n",
    "        index_to_word = [self.PAD_TOKEN, self.UNK_TOKEN]\n",
    "        index_to_vect = [None, None]\n",
    "        \n",
    "        with open(glove_file, 'r') as fp:\n",
    "            for line in fp:\n",
    "                line = line.split()\n",
    "                \n",
    "                if line[0] not in vocab:\n",
    "                    continue\n",
    "                \n",
    "                w = line[0]\n",
    "                v = np.array([float(value) for value in line[1:]])\n",
    "                \n",
    "                if self.emb_dim is None:\n",
    "                    self.emb_dim = v.shape[0]\n",
    "            \n",
    "                index_to_word.append(w)\n",
    "                index_to_vect.append(v)\n",
    "                \n",
    "        index_to_vect[0] = np.zeros(self.emb_dim)\n",
    "        index_to_vect[1] = np.mean(index_to_vect[2:], axis=0)\n",
    "    \n",
    "        self.embeddings = torch.from_numpy(np.array(index_to_vect)).float()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(self.embeddings, freeze=False)\n",
    "        \n",
    "        self.index_to_word = {i: w for i, w in enumerate(index_to_word)}\n",
    "        self.word_to_index = {w: i for i, w in self.index_to_word.items()}\n",
    "    \n",
    "    def forward(self, samples):\n",
    "        pad_ix = self.word_to_index[self.PAD_TOKEN]\n",
    "        unk_ix = self.word_to_index[self.UNK_TOKEN]\n",
    "        \n",
    "        maxlen = max([len(s) for s in samples])\n",
    "        \n",
    "        encoded = [[self.word_to_index.get(token, unk_ix) for token in tokens] for tokens in samples]\n",
    "        masks = torch.zeros(len(samples), maxlen).long()\n",
    "        \n",
    "        # Padding and masking\n",
    "        for i in range(len(encoded)):\n",
    "            masks[i, :len(encoded[i])] = 1\n",
    "            encoded[i] += [pad_ix] * max(0, (maxlen - len(encoded[i])))\n",
    "        \n",
    "        encoded = torch.tensor(encoded).long()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            encoded = encoded.cuda()\n",
    "            masks = masks.cuda()\n",
    "        \n",
    "        result = {\n",
    "            'output': self.embeddings(encoded),\n",
    "            'mask': masks,\n",
    "            'encoded': encoded\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordEmbedder(\n",
       "  (embeddings): Embedding(21695, 100)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = WordEmbedder(vocab, os.path.join(PROJ_DIR, 'glove.6B/glove.6B.100d.txt'))\n",
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded:\n",
      "tensor([[    1,   120,   698,  2756,    38,    14,     2,     1,  1500,     4,\n",
      "             1,  1076,    11,     2,   312,    16,     6,   876,   109,  7369,\n",
      "             3, 12054,     8,    45,   255,   123,     3,     6,   159,   603,\n",
      "          1464,     3,  4852,     5,   101,     3,    65,     8,    49,   312,\n",
      "             3,  8543,    14,   120,    49,  1560,    46,     1,     1, 11046,\n",
      "         13783,   268,  8244, 11046, 13783,   268,  8244,     1,     1,    38,\n",
      "           935,     1,    95,  1254],\n",
      "        [    1,     1,     1,   254,     9,   326,     1,     7,     9,   935,\n",
      "            97,  5165,   754,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "Embedding Shape:\n",
      "torch.Size([2, 64, 100])\n",
      "\n",
      "Mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Labels:\n",
      "('pos', 'neg')\n",
      "\n",
      "Tokens:\n",
      "(['I', 'just', 'recently', 'watched', 'this', 'on', 'the', 'Sundance', 'channel', '.', 'The', 'idea', 'for', 'the', 'film', 'was', 'to', 'bring', 'many', 'filmmakers', ',', 'illustrious', 'in', 'their', 'own', 'country', ',', 'to', 'make', 'short', 'films', ',', 'eleven', 'of', 'them', ',', 'all', 'in', 'one', 'film', ',', 'concentrating', 'on', 'just', 'one', 'subject', ':', 'September', '11.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'From', 'wacthing', 'this', 'movie', 'I', 'could', 'tell'], ['Ming', 'The', 'Merciless', 'does', 'a', 'little', 'Bardwork', 'and', 'a', 'movie', 'most', 'foul', '!'])\n"
     ]
    }
   ],
   "source": [
    "tokens, labels = corpus['train'][1718-1:1718+1]\n",
    "embedder_result = embedder(tokens)\n",
    "\n",
    "print(\"Encoded:\\n{}\".format(embedder_result['encoded']))\n",
    "print(\"Embedding Shape:\\n{}\\n\".format(embedder_result['output'].shape))\n",
    "print(\"Mask:\\n{}\".format(embedder_result['mask']))\n",
    "print(\"Labels:\\n{}\\n\".format(labels))\n",
    "print(\"Tokens:\\n{}\".format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, bidirectional=False, num_layers=1, drop_prob=0.3):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim // 2 if bidirectional else hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=drop_prob if num_layers > 1 else 0,\n",
    "                            batch_first=True)\n",
    "\n",
    "    def forward(self, vectors, mask):\n",
    "        batch_size = vectors.size(0)\n",
    "        max_length = vectors.size(1)\n",
    "        lengths = mask.sum(-1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(vectors)  # (batch, seq_len, num_directions * hidden_size)\n",
    "\n",
    "        assert lstm_out.size(0) == batch_size\n",
    "        assert lstm_out.size(1) == max_length\n",
    "        assert lstm_out.size(2) == self.hidden_dim\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Separate the directions of the LSTM\n",
    "            lstm_out = lstm_out.view(batch_size, max_length, 2, self.hidden_dim // 2)\n",
    "\n",
    "            # Pick up the last hidden state per direction\n",
    "            fw_last_hn = lstm_out[range(batch_size), lengths - 1, 0]  # (batch, hidden // 2)\n",
    "            bw_last_hn = lstm_out[range(batch_size), 0, 1]            # (batch, hidden // 2)\n",
    "\n",
    "            last_hn = torch.cat([fw_last_hn, bw_last_hn], dim=1)      # (batch, hidden // 2) -> (batch, hidden)\n",
    "        else:\n",
    "            last_hn = lstm_out[range(batch_size), lengths - 1]        # (batch, hidden)\n",
    "\n",
    "        return {'output': last_hn, 'outputs': lstm_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMLayer(\n",
       "  (lstm): LSTM(100, 64, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_layer = LSTMLayer(embedder.emb_dim, 64)\n",
    "lstm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.6868e-02,  8.1113e-02, -5.3292e-02,  6.6632e-03, -2.4285e-01,\n",
      "         -1.3585e-01, -1.8223e-01,  5.1247e-02, -4.3143e-01, -7.2476e-02,\n",
      "         -1.5521e-01, -1.3031e-01,  1.5079e-01,  5.0297e-02,  1.7093e-01,\n",
      "          2.0144e-01, -5.0039e-02,  7.8581e-02, -1.8033e-01, -7.7575e-02,\n",
      "          1.5991e-01,  1.7170e-01, -2.7888e-01,  1.2124e-01,  6.5989e-02,\n",
      "          6.6409e-02,  6.9906e-02, -1.9961e-01,  2.2071e-01,  1.0483e-01,\n",
      "          5.9449e-02, -6.1370e-02, -1.4324e-01, -2.2716e-02,  1.9821e-01,\n",
      "         -4.2869e-02, -7.4581e-02,  1.7959e-04,  2.1500e-01,  5.6030e-02,\n",
      "         -2.0585e-01,  1.1483e-01, -1.9692e-01,  9.8408e-02,  6.4696e-02,\n",
      "         -1.4615e-02,  2.8201e-02,  7.4419e-02,  2.9322e-01, -9.9421e-02,\n",
      "         -1.1486e-01, -2.3422e-02, -5.7116e-02,  2.3961e-01, -1.7119e-01,\n",
      "          1.9581e-01, -7.9659e-02,  8.1953e-02,  1.2482e-01,  1.1906e-01,\n",
      "          4.4853e-02,  1.3501e-01,  8.4314e-02,  1.6695e-01],\n",
      "        [-1.7681e-01,  3.6196e-02,  2.0196e-03,  9.5723e-02, -1.4305e-01,\n",
      "         -6.7923e-02, -8.4490e-02,  3.8366e-02, -2.9563e-01, -2.2804e-01,\n",
      "         -1.5381e-01, -2.2727e-01,  3.7376e-02,  5.0685e-02,  2.5234e-01,\n",
      "          2.9970e-01, -5.5062e-02, -6.7035e-03, -3.4600e-01, -1.1731e-01,\n",
      "          6.0688e-02,  4.9888e-02, -2.6947e-01,  1.1460e-01,  2.2192e-02,\n",
      "         -7.3232e-02, -9.1241e-03, -1.6241e-01,  2.4140e-01,  9.3786e-02,\n",
      "          1.5984e-01, -7.6725e-02, -1.5809e-01,  2.2278e-02,  1.4415e-01,\n",
      "         -7.6935e-02, -1.0167e-01,  3.9346e-02,  2.9782e-01, -8.7097e-02,\n",
      "         -2.1322e-01,  1.7165e-01,  8.6421e-02,  1.8440e-01, -5.0497e-02,\n",
      "         -1.6424e-01,  2.9890e-02, -1.7166e-02,  2.5611e-01, -1.4473e-01,\n",
      "         -4.6314e-02, -1.5958e-01, -5.3917e-02,  2.0515e-01, -6.5182e-02,\n",
      "          2.3980e-02,  1.3209e-02,  5.5894e-02, -6.7373e-02, -1.8257e-01,\n",
      "          1.0261e-01,  8.8221e-02,  5.2314e-02,  1.5718e-01]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "lstm_result = lstm_layer(embedder_result['output'], embedder_result['mask'])\n",
    "print(lstm_result['output'])\n",
    "print(lstm_result['output'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classifier (putting all together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedder, extractor):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.extractor = extractor\n",
    "        self.classifier = nn.Linear(extractor.hidden_dim, 1)\n",
    "        self.xentropy = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, tokens, targets=None):\n",
    "        embedded = self.embedder(tokens)\n",
    "        extracted = self.extractor(embedded['output'], embedded['mask'])\n",
    "        \n",
    "        logits = self.classifier(extracted['output'])\n",
    "        loss = None\n",
    "        \n",
    "        if targets is not None:\n",
    "            logits = logits.view(-1)\n",
    "            targets = targets.float()\n",
    "            loss = self.xentropy(logits, targets)            \n",
    "        \n",
    "        return {'output': logits, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(21695, 100)\n",
       "  )\n",
       "  (extractor): LSTMLayer(\n",
       "    (lstm): LSTM(100, 64, batch_first=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (xentropy): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTMClassifier(embedder, lstm_layer)\n",
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embd params:  2,169,500\n",
      "LSTM params:  42,496\n",
      "Clfr params:  65\n",
      "Total params: 2,212,061\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import count_params\n",
    "    \n",
    "print(\"Embd params:  {:,}\".format(count_params(lstm_model.embedder)))\n",
    "print(\"LSTM params:  {:,}\".format(count_params(lstm_model.extractor)))\n",
    "print(\"Clfr params:  {:,}\".format(count_params(lstm_model.classifier)))\n",
    "print(\"Total params: {:,}\".format(count_params(lstm_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.6909, grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'output': tensor([0.0624, 0.0736], grad_fn=<ViewBackward>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_tokens = ['this is bad'.split(), 'this is not bad !'.split()]\n",
    "dummy_labels = torch.tensor([0, 1]).long()\n",
    "\n",
    "result = lstm_model(dummy_tokens, dummy_labels)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1.0, 1.0]\n",
      "Probs: [0.5155900716781616, 0.5183917880058289]\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import process_logits\n",
    "\n",
    "preds, probs = process_logits(result['output'])\n",
    "\n",
    "print(\"Preds: {}\".format(preds))\n",
    "print(\"Probs: {}\".format(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
