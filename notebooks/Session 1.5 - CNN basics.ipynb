{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PROJ_DIR = os.path.join(os.environ['WORKSPACE'], 'tutorial/')\n",
    "\n",
    "if PROJ_DIR not in sys.path:\n",
    "    sys.path.append(PROJ_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.dataset import IMDBDatset\n",
    "from src.utilities import flatten, get_dataloader\n",
    "\n",
    "with open('data.pickle', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)\n",
    " \n",
    "dataloaders = {\n",
    "    'train': get_dataloader(corpus['train'], batch_size=32, shuffle=True),\n",
    "    'dev':   get_dataloader(corpus['dev'],   batch_size=128, shuffle=False),\n",
    "    'test':  get_dataloader(corpus['test'],  batch_size=128, shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, channels, kernels, maxlen):\n",
    "        super(CNNLayer, self).__init__()\n",
    "        \n",
    "        assert len(kernels) == len(channels)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.maxlen = maxlen      # maximum sequence length\n",
    "        self.kernels = kernels    # playing the role of n-gram of different orders\n",
    "        self.channels = channels  # the number of output channels per convolution layer\n",
    "\n",
    "        self.cnn = {}\n",
    "        self.bn = {}\n",
    "\n",
    "        for kernel, out_channels in zip(kernels, channels):\n",
    "            self.cnn[f'{kernel}_gram'] = nn.Conv1d(self.input_dim, out_channels, kernel)\n",
    "            self.bn[f'{kernel}_gram'] = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.cnn = nn.ModuleDict(self.cnn)\n",
    "        self.bn = nn.ModuleDict(self.bn)\n",
    "\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        batch_size = embeddings.size(0)\n",
    "        seq_length = embeddings.size(1)\n",
    "        seq_maxlen = min(seq_length, self.maxlen)\n",
    "\n",
    "        # Prepare for sliding the Conv1d across time\n",
    "        embeddings = embeddings.transpose(1, 2) # -> (batch, embedding, seq_length)\n",
    "\n",
    "        convs = []\n",
    "        for kernel, channels in zip(self.kernels, self.channels):\n",
    "            cnn_key = f'{kernel}_gram'\n",
    "\n",
    "            convolved = self.cnn[cnn_key](embeddings)           # -> (batch, n_filters, channels)\n",
    "\n",
    "            curr_shape = convolved.size()\n",
    "            expt_shape = (batch_size, channels, seq_maxlen - kernel + 1)\n",
    "            \n",
    "            assert curr_shape == expt_shape, \"Wrong size: {}. Expected {}\".format(curr_shape, expt_shape)\n",
    "\n",
    "            convolved = self.bn[cnn_key](convolved)             # -> (batch, n_filters, channels)\n",
    "            convolved, _ = torch.max(convolved, dim=2)          # -> (batch, n_filters)\n",
    "            convolved = torch.nn.functional.relu(convolved)\n",
    "            convs.append(convolved)\n",
    "\n",
    "        convs = torch.cat(convs, dim=1)  # -> (batch, sum(n_filters))  dim 1 is the sum of n_filters from all cnn layers\n",
    "\n",
    "        return {'output': convs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedder, extractor):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.extractor = extractor\n",
    "        self.classifier = nn.Linear(sum(extractor.channels), 1)\n",
    "        self.xentropy = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, tokens, targets=None):\n",
    "        embedded = self.embedder(tokens)\n",
    "        extracted = self.extractor(embedded['output'])\n",
    "        \n",
    "        logits = self.classifier(extracted['output'])\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.view(-1)\n",
    "            targets = targets.float()\n",
    "            loss = self.xentropy(logits, targets)\n",
    "\n",
    "        return {'output': logits, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNClassifier(\n",
       "  (embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(21695, 100)\n",
       "  )\n",
       "  (extractor): CNNLayer(\n",
       "    (cnn): ModuleDict(\n",
       "      (2_gram): Conv1d(100, 32, kernel_size=(2,), stride=(1,))\n",
       "      (3_gram): Conv1d(100, 64, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (bn): ModuleDict(\n",
       "      (2_gram): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3_gram): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=96, out_features=1, bias=True)\n",
       "  (xentropy): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.nets.embedder import WordEmbedder\n",
    "\n",
    "vocab = set(flatten(corpus['train'].tokens + corpus['dev'].tokens))\n",
    "\n",
    "def create_cnn_classifier():\n",
    "    embedder = WordEmbedder(vocab, os.path.join(PROJ_DIR, 'glove.6B/glove.6B.100d.txt'))\n",
    "    extractor = CNNLayer(embedder.emb_dim, channels=[32, 64], kernels=[2, 3], maxlen=64)\n",
    "    cnn_model = CNNClassifier(embedder, extractor)\n",
    "    return cnn_model\n",
    "\n",
    "model = create_cnn_classifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E001 [TRAIN] Loss: 0.6406, Acc: 0.6442 [DEV] Loss: 0.5253, Acc: 0.7400 [TEST] Loss: 0.5271, Acc: 0.7308 * \n",
      "E002 [TRAIN] Loss: 0.5478, Acc: 0.7209 [DEV] Loss: 0.5075, Acc: 0.7442 [TEST] Loss: 0.5114, Acc: 0.7447 * \n",
      "E003 [TRAIN] Loss: 0.5038, Acc: 0.7507 [DEV] Loss: 0.5009, Acc: 0.7472 [TEST] Loss: 0.4998, Acc: 0.7554 * \n",
      "E004 [TRAIN] Loss: 0.4881, Acc: 0.7603 [DEV] Loss: 0.6079, Acc: 0.6618 [TEST] Loss: 0.5931, Acc: 0.6702\n",
      "E005 [TRAIN] Loss: 0.5009, Acc: 0.7507 [DEV] Loss: 0.5663, Acc: 0.7106 [TEST] Loss: 0.5564, Acc: 0.7081\n",
      "E006 [TRAIN] Loss: 0.4443, Acc: 0.7915 [DEV] Loss: 0.5137, Acc: 0.7456 [TEST] Loss: 0.5240, Acc: 0.7445\n",
      "E007 [TRAIN] Loss: 0.4071, Acc: 0.8114 [DEV] Loss: 0.5381, Acc: 0.7484 [TEST] Loss: 0.5500, Acc: 0.7456 * \n",
      "E008 [TRAIN] Loss: 0.3776, Acc: 0.8299 [DEV] Loss: 0.5451, Acc: 0.7464 [TEST] Loss: 0.5561, Acc: 0.7399\n",
      "E009 [TRAIN] Loss: 0.3588, Acc: 0.8390 [DEV] Loss: 0.5985, Acc: 0.7412 [TEST] Loss: 0.6113, Acc: 0.7449\n",
      "E010 [TRAIN] Loss: 0.3394, Acc: 0.8506 [DEV] Loss: 0.5793, Acc: 0.7314 [TEST] Loss: 0.5879, Acc: 0.7326\n",
      "Done training!\n",
      "Returning best model from epoch 7 with loss 0.53811 and accuracy 0.74840\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import train\n",
    "import torch.optim as optim\n",
    "\n",
    "config = {\n",
    "    'lr': 1e-2,\n",
    "    'momentum': 0.99,\n",
    "    'epochs': 10,\n",
    "    'checkpoint': 'cnn_model.pt'\n",
    "}\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'])\n",
    "model = train(model, dataloaders, optimizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
