{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PROJ_DIR = os.path.join(os.environ['WORKSPACE'], 'tutorial/')\n",
    "\n",
    "if PROJ_DIR not in sys.path:\n",
    "    sys.path.append(PROJ_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.dataset import IMDBDatset\n",
    "from src.utilities import flatten, get_dataloader\n",
    "\n",
    "with open('data.pickle', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)\n",
    " \n",
    "dataloaders = {\n",
    "    'train': get_dataloader(corpus['train'], batch_size=32, shuffle=True),\n",
    "    'dev':   get_dataloader(corpus['dev'],   batch_size=128, shuffle=False),\n",
    "    'test':  get_dataloader(corpus['test'],  batch_size=128, shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Attention mechanism: a = softmax(v' Â· tanh(W_h h + b_h))\"\"\"\n",
    "    def __init__(self,  hidden_size, attn_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.dh = hidden_size\n",
    "        self.da = attn_size\n",
    "\n",
    "        self.W = nn.Linear(self.dh, self.da)        # (feat_dim, attn_dim)\n",
    "        self.v = nn.Linear(self.da, 1)              # (attn_dim, 1)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        # Raw scores\n",
    "        u = self.v(torch.tanh(self.W(inputs)))      # (batch, seq, hidden) -> (batch, seq, attn) -> (batch, seq, 1)\n",
    "\n",
    "        # Masked softmax\n",
    "        u = u.exp()                                 # exp to calculate softmax\n",
    "        u = mask.unsqueeze(2).float() * u           # (batch, seq, 1) * (batch, seq, 1) to zerout out-of-mask numbers\n",
    "        sums = torch.sum(u, dim=1, keepdim=True)    # now we are sure only in-mask values are in sum\n",
    "        a = u / sums                                # the probability distribution only goes to in-mask values now\n",
    "\n",
    "        # Weighted sum of the input vectors\n",
    "        z = torch.sum(inputs * a, dim=1)\n",
    "        \n",
    "        return  {'output':z, 'attention': a.view(inputs.size(0), inputs.size(1))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Attention  Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, embedder, extractor, attention):\n",
    "        super(LSTMAttentionClassifier, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.extractor = extractor\n",
    "        self.attention = attention\n",
    "        self.classifier = nn.Linear(extractor.hidden_dim, 1)\n",
    "        self.xentropy = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, tokens, targets=None):\n",
    "        embedded = self.embedder(tokens)\n",
    "        extracted = self.extractor(embedded['output'], embedded['mask'])\n",
    "        attended = self.attention(extracted['outputs'], embedded['mask'])\n",
    "        \n",
    "        logits = self.classifier(attended['output'])\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.view(-1)\n",
    "            targets = targets.float()\n",
    "            loss = self.xentropy(logits, targets)\n",
    "\n",
    "        return {'output': logits, 'loss': loss, 'attention': attended['attention'].data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMAttentionClassifier(\n",
       "  (embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(21695, 100)\n",
       "  )\n",
       "  (extractor): LSTMLayer(\n",
       "    (lstm): LSTM(100, 64, batch_first=True)\n",
       "  )\n",
       "  (attention): AttentionLayer(\n",
       "    (W): Linear(in_features=64, out_features=100, bias=True)\n",
       "    (v): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (xentropy): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.nets.embedder import WordEmbedder\n",
    "from src.nets.lstm import LSTMLayer\n",
    "from src.nets.classifier import LSTMClassifier\n",
    "\n",
    "vocab = set(flatten(corpus['train'].tokens + corpus['dev'].tokens))\n",
    "\n",
    "def create_lstm_attention_classifier():\n",
    "    embedder = WordEmbedder(vocab, os.path.join(PROJ_DIR, 'glove.6B/glove.6B.100d.txt'))\n",
    "    lstm_layer = LSTMLayer(embedder.emb_dim, hidden_dim=64, bidirectional=False, num_layers=1)\n",
    "    attn_layer = AttentionLayer(hidden_size=64, attn_size=100)\n",
    "    lstm_attn_model = LSTMAttentionClassifier(embedder, lstm_layer, attn_layer)\n",
    "    return lstm_attn_model\n",
    "\n",
    "model = create_lstm_attention_classifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E001 [TRAIN] Loss: 0.6854, Acc: 0.5496 [DEV] Loss: 0.6502, Acc: 0.6330 [TEST] Loss: 0.6579, Acc: 0.6076 * \n",
      "E002 [TRAIN] Loss: 0.6326, Acc: 0.6387 [DEV] Loss: 0.6107, Acc: 0.6586 [TEST] Loss: 0.6001, Acc: 0.6711 * \n",
      "E003 [TRAIN] Loss: 0.5470, Acc: 0.7193 [DEV] Loss: 0.5093, Acc: 0.7510 [TEST] Loss: 0.5114, Acc: 0.7449 * \n",
      "E004 [TRAIN] Loss: 0.4996, Acc: 0.7516 [DEV] Loss: 0.4836, Acc: 0.7676 [TEST] Loss: 0.4851, Acc: 0.7572 * \n",
      "E005 [TRAIN] Loss: 0.4706, Acc: 0.7705 [DEV] Loss: 0.4716, Acc: 0.7634 [TEST] Loss: 0.4759, Acc: 0.7646\n",
      "E006 [TRAIN] Loss: 0.4510, Acc: 0.7841 [DEV] Loss: 0.4915, Acc: 0.7516 [TEST] Loss: 0.4943, Acc: 0.7602\n",
      "E007 [TRAIN] Loss: 0.4455, Acc: 0.7877 [DEV] Loss: 0.4902, Acc: 0.7528 [TEST] Loss: 0.4954, Acc: 0.7626\n",
      "E008 [TRAIN] Loss: 0.4179, Acc: 0.8097 [DEV] Loss: 0.4738, Acc: 0.7720 [TEST] Loss: 0.4704, Acc: 0.7716 * \n",
      "E009 [TRAIN] Loss: 0.3847, Acc: 0.8232 [DEV] Loss: 0.4749, Acc: 0.7706 [TEST] Loss: 0.4720, Acc: 0.7733\n",
      "E010 [TRAIN] Loss: 0.3558, Acc: 0.8433 [DEV] Loss: 0.5193, Acc: 0.7632 [TEST] Loss: 0.5115, Acc: 0.7691\n",
      "Done training!\n",
      "Returning best model from epoch 8 with loss 0.47378 and accuracy 0.77200\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import train\n",
    "import torch.optim as optim\n",
    "\n",
    "config = {\n",
    "    'lr': 1e-2,\n",
    "    'momentum': 0.99,\n",
    "    'epochs': 10,\n",
    "    'checkpoint': 'lstm_attn_model.pt'\n",
    "}\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'])\n",
    "model = train(model, dataloaders, optimizer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from src.utilities import process_logits\n",
    "\n",
    "def attn_model_predict(model, dataset):\n",
    "    loss = 0\n",
    "    probs, preds, truth = [], [], []\n",
    "    attns = []\n",
    "\n",
    "    model.eval()\n",
    "    for tokens, targets in dataset:\n",
    "        result = model(tokens, targets)\n",
    "\n",
    "        batch_preds, batch_probs = process_logits(result['output'])\n",
    "        loss += result['loss'].item() * len(batch_preds)\n",
    "\n",
    "        preds += batch_preds\n",
    "        probs += batch_probs\n",
    "        truth += targets.data.cpu().tolist()\n",
    "        attns += result['attention'].cpu().tolist()\n",
    "\n",
    "    loss /= len(truth)\n",
    "    acc = accuracy_score(truth, preds)\n",
    "    print(\"Loss: {:.5f}, Acc: {:.5f}\".format(loss, acc))\n",
    "\n",
    "    return probs, preds, truth, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.47378, Acc: 0.77200\n"
     ]
    }
   ],
   "source": [
    "probs, preds, truth, attns = attn_model_predict(model, dataloaders['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00749 The\n",
      "0.01684 characters\n",
      "0.00437 are\n",
      "0.02744 unlikeable\n",
      "0.02073 and\n",
      "0.00791 the\n",
      "0.08742 script\n",
      "0.03646 is\n",
      "0.29595 awful\n",
      "0.10089 .\n",
      "0.04331 It\n",
      "0.01325 's\n",
      "0.00459 a\n",
      "0.11016 waste\n",
      "0.06131 of\n",
      "0.02121 the\n",
      "0.04267 talents\n",
      "0.03129 of\n",
      "0.01442 Deneuve\n",
      "0.02232 and\n",
      "0.01114 Auteuil\n",
      "0.01881 .\n",
      "\n",
      "Sample 1767\n",
      "Pred: 0.0 (Prob: 0.00752)\n",
      "Truth: 0\n",
      "Attn: 1.00000\n",
      "Size: 22\n"
     ]
    }
   ],
   "source": [
    "for i, tokens in enumerate(dataloaders['dev'].dataset.tokens):\n",
    "    if len(tokens) < 25:\n",
    "        attn_total = 0\n",
    "        for token, attn in zip(tokens, attns[i]):\n",
    "            attn_total += attn\n",
    "            print('{:.5f} {}'.format(attn, token))\n",
    "        print(f'\\nSample {i}')\n",
    "        print(f'Pred: {preds[i]} (Prob: {probs[i]:.5f})')\n",
    "        print(f'Truth: {truth[i]}')\n",
    "        print(f'Attn: {attn_total:.5f}')\n",
    "        print(f'Size: {len(tokens)}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
